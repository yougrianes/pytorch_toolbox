{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is always a problem of how do we setup a loss function and optimize it. Choosing the right loss function increase the chances of model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.tensor>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = torch.tensor([0.,1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11])\n",
    "t_u = torch.tensor([0.4,1.2,2.5,3.9,4.3,5.7,6.2,7.9,8.3,9.5,10.5,11.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4000,  1.2000,  2.5000,  3.9000,  4.3000,  5.7000,  6.2000,  7.9000,\n",
       "         8.3000,  9.5000, 10.5000, 11.5000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_u"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define model, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.ones(1)\n",
    "b = torch.zeros(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model infer of training set tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4000,  1.2000,  2.5000,  3.9000,  4.3000,  5.7000,  6.2000,  7.9000,\n",
       "         8.3000,  9.5000, 10.5000, 11.5000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2942)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count loss function value\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors in the first round of iteration is backpropagated to reduce the error in the second round, for which the initial set of weight should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0976)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 0.1\n",
    "loss_with_delta = loss_fn(model(t_u, w+delta, b), t_c)\n",
    "loss_before_delta = loss_fn(model(t_u, w, b), t_c)\n",
    "loss_rate_of_change_w = (loss_with_delta - loss_before_delta)\n",
    "loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "w = w - learning_rate * loss_rate_of_change_w\n",
    "\n",
    "# first, delta > 0, so, if loss_rate_of_change_w > 0, then we can realize that the optimization could go to a bad direction,\n",
    "# and the weight(w) should avoid of this direction. \n",
    "# if loss_rate_of_change_w < 0, the w should grow, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "learning_rate = 1e-2\n",
    "\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, w+delta,b), t_c) - loss_fn(model(t_u, w-delta, b), t_c)) / (2.0*delta)\n",
    "w = w - learning_rate * loss_rate_of_change_w\n",
    "\n",
    "loss_rate_of_change_b =  (loss_fn(model(t_u, w,b+delta), t_c) - loss_fn(model(t_u, w, b-delta), t_c)) / (2.0*delta)\n",
    "b = b - learning_rate * loss_rate_of_change_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9379]), tensor([-0.0024]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = nn.MSELoss()\n",
    "input = torch.randn(10,5, requires_grad = True)\n",
    "target = torch.randn(10,5)\n",
    "output = loss(input,target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5577, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MseLossBackward0 at 0x260da947220>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.grad_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we estimate the derivative of a loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c)\n",
    "    return dsq_diffs\n",
    "\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0\n",
    "\n",
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_fn(t_p, t_c) * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.mean(), dloss_db.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 2.2774999141693115\n",
      "params: tensor([1., 1.]), Grad: tensor([18.1550,  2.9833])\n",
      "Epoch 1: Loss 0.5418065786361694\n",
      "params: tensor([0.8184, 0.9702]), Grad: tensor([0.3527, 0.7481])\n",
      "Epoch 2: Loss 0.5359362959861755\n",
      "params: tensor([0.8149, 0.9627]), Grad: tensor([-0.0759,  0.6909])\n",
      "Epoch 3: Loss 0.5311182141304016\n",
      "params: tensor([0.8157, 0.9558]), Grad: tensor([-0.0858,  0.6861])\n",
      "Epoch 4: Loss 0.5263487100601196\n",
      "params: tensor([0.8165, 0.9489]), Grad: tensor([-0.0856,  0.6827])\n",
      "Epoch 5: Loss 0.5216265320777893\n",
      "params: tensor([0.8174, 0.9421]), Grad: tensor([-0.0852,  0.6793])\n",
      "Epoch 6: Loss 0.5169512629508972\n",
      "params: tensor([0.8182, 0.9353]), Grad: tensor([-0.0847,  0.6759])\n",
      "Epoch 7: Loss 0.5123223662376404\n",
      "params: tensor([0.8191, 0.9285]), Grad: tensor([-0.0843,  0.6726])\n",
      "Epoch 8: Loss 0.5077394843101501\n",
      "params: tensor([0.8199, 0.9218]), Grad: tensor([-0.0839,  0.6692])\n",
      "Epoch 9: Loss 0.5032020807266235\n",
      "params: tensor([0.8208, 0.9151]), Grad: tensor([-0.0835,  0.6659])\n",
      "Epoch 10: Loss 0.49870970845222473\n",
      "params: tensor([0.8216, 0.9085]), Grad: tensor([-0.0831,  0.6626])\n",
      "Epoch 11: Loss 0.4942619502544403\n",
      "params: tensor([0.8224, 0.9018]), Grad: tensor([-0.0826,  0.6593])\n",
      "Epoch 12: Loss 0.48985859751701355\n",
      "params: tensor([0.8233, 0.8952]), Grad: tensor([-0.0822,  0.6560])\n",
      "Epoch 13: Loss 0.48549869656562805\n",
      "params: tensor([0.8241, 0.8887]), Grad: tensor([-0.0818,  0.6527])\n",
      "Epoch 14: Loss 0.4811820685863495\n",
      "params: tensor([0.8249, 0.8822]), Grad: tensor([-0.0814,  0.6495])\n",
      "Epoch 15: Loss 0.47690847516059875\n",
      "params: tensor([0.8257, 0.8757]), Grad: tensor([-0.0810,  0.6462])\n",
      "Epoch 16: Loss 0.47267723083496094\n",
      "params: tensor([0.8265, 0.8692]), Grad: tensor([-0.0806,  0.6430])\n",
      "Epoch 17: Loss 0.46848806738853455\n",
      "params: tensor([0.8273, 0.8628]), Grad: tensor([-0.0802,  0.6398])\n",
      "Epoch 18: Loss 0.4643404185771942\n",
      "params: tensor([0.8281, 0.8564]), Grad: tensor([-0.0798,  0.6366])\n",
      "Epoch 19: Loss 0.46023404598236084\n",
      "params: tensor([0.8289, 0.8500]), Grad: tensor([-0.0794,  0.6335])\n",
      "Epoch 20: Loss 0.4561683237552643\n",
      "params: tensor([0.8297, 0.8437]), Grad: tensor([-0.0790,  0.6303])\n",
      "Epoch 21: Loss 0.4521430730819702\n",
      "params: tensor([0.8305, 0.8374]), Grad: tensor([-0.0786,  0.6272])\n",
      "Epoch 22: Loss 0.44815778732299805\n",
      "params: tensor([0.8313, 0.8311]), Grad: tensor([-0.0782,  0.6240])\n",
      "Epoch 23: Loss 0.4442119896411896\n",
      "params: tensor([0.8321, 0.8249]), Grad: tensor([-0.0778,  0.6209])\n",
      "Epoch 24: Loss 0.44030535221099854\n",
      "params: tensor([0.8329, 0.8186]), Grad: tensor([-0.0775,  0.6179])\n",
      "Epoch 25: Loss 0.436437726020813\n",
      "params: tensor([0.8336, 0.8125]), Grad: tensor([-0.0771,  0.6148])\n",
      "Epoch 26: Loss 0.43260833621025085\n",
      "params: tensor([0.8344, 0.8063]), Grad: tensor([-0.0767,  0.6117])\n",
      "Epoch 27: Loss 0.4288170337677002\n",
      "params: tensor([0.8352, 0.8002]), Grad: tensor([-0.0763,  0.6087])\n",
      "Epoch 28: Loss 0.42506346106529236\n",
      "params: tensor([0.8359, 0.7941]), Grad: tensor([-0.0759,  0.6056])\n",
      "Epoch 29: Loss 0.42134711146354675\n",
      "params: tensor([0.8367, 0.7881]), Grad: tensor([-0.0755,  0.6026])\n",
      "Epoch 30: Loss 0.4176676273345947\n",
      "params: tensor([0.8375, 0.7820]), Grad: tensor([-0.0752,  0.5996])\n",
      "Epoch 31: Loss 0.41402459144592285\n",
      "params: tensor([0.8382, 0.7760]), Grad: tensor([-0.0748,  0.5966])\n",
      "Epoch 32: Loss 0.41041770577430725\n",
      "params: tensor([0.8390, 0.7701]), Grad: tensor([-0.0744,  0.5937])\n",
      "Epoch 33: Loss 0.4068469703197479\n",
      "params: tensor([0.8397, 0.7641]), Grad: tensor([-0.0741,  0.5907])\n",
      "Epoch 34: Loss 0.4033113420009613\n",
      "params: tensor([0.8404, 0.7582]), Grad: tensor([-0.0737,  0.5878])\n",
      "Epoch 35: Loss 0.3998109996318817\n",
      "params: tensor([0.8412, 0.7523]), Grad: tensor([-0.0733,  0.5849])\n",
      "Epoch 36: Loss 0.3963453769683838\n",
      "params: tensor([0.8419, 0.7465]), Grad: tensor([-0.0730,  0.5819])\n",
      "Epoch 37: Loss 0.39291414618492126\n",
      "params: tensor([0.8426, 0.7407]), Grad: tensor([-0.0726,  0.5790])\n",
      "Epoch 38: Loss 0.3895169496536255\n",
      "params: tensor([0.8434, 0.7349]), Grad: tensor([-0.0722,  0.5762])\n",
      "Epoch 39: Loss 0.38615357875823975\n",
      "params: tensor([0.8441, 0.7291]), Grad: tensor([-0.0719,  0.5733])\n",
      "Epoch 40: Loss 0.38282355666160583\n",
      "params: tensor([0.8448, 0.7234]), Grad: tensor([-0.0715,  0.5704])\n",
      "Epoch 41: Loss 0.37952664494514465\n",
      "params: tensor([0.8455, 0.7177]), Grad: tensor([-0.0712,  0.5676])\n",
      "Epoch 42: Loss 0.3762625455856323\n",
      "params: tensor([0.8462, 0.7120]), Grad: tensor([-0.0708,  0.5648])\n",
      "Epoch 43: Loss 0.3730306625366211\n",
      "params: tensor([0.8469, 0.7064]), Grad: tensor([-0.0704,  0.5620])\n",
      "Epoch 44: Loss 0.3698309659957886\n",
      "params: tensor([0.8477, 0.7007]), Grad: tensor([-0.0701,  0.5592])\n",
      "Epoch 45: Loss 0.3666630983352661\n",
      "params: tensor([0.8484, 0.6952]), Grad: tensor([-0.0698,  0.5564])\n",
      "Epoch 46: Loss 0.3635265827178955\n",
      "params: tensor([0.8490, 0.6896]), Grad: tensor([-0.0694,  0.5536])\n",
      "Epoch 47: Loss 0.3604213297367096\n",
      "params: tensor([0.8497, 0.6841]), Grad: tensor([-0.0691,  0.5509])\n",
      "Epoch 48: Loss 0.35734668374061584\n",
      "params: tensor([0.8504, 0.6785]), Grad: tensor([-0.0687,  0.5481])\n",
      "Epoch 49: Loss 0.3543027937412262\n",
      "params: tensor([0.8511, 0.6731]), Grad: tensor([-0.0684,  0.5454])\n",
      "Epoch 50: Loss 0.3512890636920929\n",
      "params: tensor([0.8518, 0.6676]), Grad: tensor([-0.0680,  0.5427])\n",
      "Epoch 51: Loss 0.3483053743839264\n",
      "params: tensor([0.8525, 0.6622]), Grad: tensor([-0.0677,  0.5400])\n",
      "Epoch 52: Loss 0.3453512191772461\n",
      "params: tensor([0.8532, 0.6568]), Grad: tensor([-0.0674,  0.5373])\n",
      "Epoch 53: Loss 0.3424263000488281\n",
      "params: tensor([0.8538, 0.6514]), Grad: tensor([-0.0670,  0.5346])\n",
      "Epoch 54: Loss 0.33953049778938293\n",
      "params: tensor([0.8545, 0.6461]), Grad: tensor([-0.0667,  0.5319])\n",
      "Epoch 55: Loss 0.3366635739803314\n",
      "params: tensor([0.8552, 0.6407]), Grad: tensor([-0.0664,  0.5293])\n",
      "Epoch 56: Loss 0.3338249921798706\n",
      "params: tensor([0.8558, 0.6355]), Grad: tensor([-0.0660,  0.5267])\n",
      "Epoch 57: Loss 0.3310147225856781\n",
      "params: tensor([0.8565, 0.6302]), Grad: tensor([-0.0657,  0.5240])\n",
      "Epoch 58: Loss 0.3282322883605957\n",
      "params: tensor([0.8572, 0.6249]), Grad: tensor([-0.0654,  0.5214])\n",
      "Epoch 59: Loss 0.32547736167907715\n",
      "params: tensor([0.8578, 0.6197]), Grad: tensor([-0.0650,  0.5188])\n",
      "Epoch 60: Loss 0.322750061750412\n",
      "params: tensor([0.8585, 0.6145]), Grad: tensor([-0.0647,  0.5163])\n",
      "Epoch 61: Loss 0.32004955410957336\n",
      "params: tensor([0.8591, 0.6094]), Grad: tensor([-0.0644,  0.5137])\n",
      "Epoch 62: Loss 0.3173759877681732\n",
      "params: tensor([0.8597, 0.6042]), Grad: tensor([-0.0641,  0.5111])\n",
      "Epoch 63: Loss 0.3147289752960205\n",
      "params: tensor([0.8604, 0.5991]), Grad: tensor([-0.0638,  0.5086])\n",
      "Epoch 64: Loss 0.31210824847221375\n",
      "params: tensor([0.8610, 0.5940]), Grad: tensor([-0.0634,  0.5061])\n",
      "Epoch 65: Loss 0.30951371788978577\n",
      "params: tensor([0.8617, 0.5890]), Grad: tensor([-0.0631,  0.5035])\n",
      "Epoch 66: Loss 0.3069446086883545\n",
      "params: tensor([0.8623, 0.5840]), Grad: tensor([-0.0628,  0.5010])\n",
      "Epoch 67: Loss 0.30440112948417664\n",
      "params: tensor([0.8629, 0.5789]), Grad: tensor([-0.0625,  0.4985])\n",
      "Epoch 68: Loss 0.30188310146331787\n",
      "params: tensor([0.8635, 0.5740]), Grad: tensor([-0.0622,  0.4961])\n",
      "Epoch 69: Loss 0.2993897795677185\n",
      "params: tensor([0.8642, 0.5690]), Grad: tensor([-0.0619,  0.4936])\n",
      "Epoch 70: Loss 0.29692143201828003\n",
      "params: tensor([0.8648, 0.5641]), Grad: tensor([-0.0616,  0.4911])\n",
      "Epoch 71: Loss 0.29447758197784424\n",
      "params: tensor([0.8654, 0.5591]), Grad: tensor([-0.0613,  0.4887])\n",
      "Epoch 72: Loss 0.29205796122550964\n",
      "params: tensor([0.8660, 0.5543]), Grad: tensor([-0.0610,  0.4863])\n",
      "Epoch 73: Loss 0.28966230154037476\n",
      "params: tensor([0.8666, 0.5494]), Grad: tensor([-0.0607,  0.4838])\n",
      "Epoch 74: Loss 0.2872905135154724\n",
      "params: tensor([0.8672, 0.5446]), Grad: tensor([-0.0603,  0.4814])\n",
      "Epoch 75: Loss 0.2849422097206116\n",
      "params: tensor([0.8678, 0.5397]), Grad: tensor([-0.0601,  0.4790])\n",
      "Epoch 76: Loss 0.2826172709465027\n",
      "params: tensor([0.8684, 0.5350]), Grad: tensor([-0.0598,  0.4766])\n",
      "Epoch 77: Loss 0.28031548857688904\n",
      "params: tensor([0.8690, 0.5302]), Grad: tensor([-0.0595,  0.4743])\n",
      "Epoch 78: Loss 0.2780363857746124\n",
      "params: tensor([0.8696, 0.5254]), Grad: tensor([-0.0592,  0.4719])\n",
      "Epoch 79: Loss 0.27578017115592957\n",
      "params: tensor([0.8702, 0.5207]), Grad: tensor([-0.0589,  0.4696])\n",
      "Epoch 80: Loss 0.2735462486743927\n",
      "params: tensor([0.8708, 0.5160]), Grad: tensor([-0.0586,  0.4672])\n",
      "Epoch 81: Loss 0.27133435010910034\n",
      "params: tensor([0.8714, 0.5114]), Grad: tensor([-0.0583,  0.4649])\n",
      "Epoch 82: Loss 0.26914459466934204\n",
      "params: tensor([0.8720, 0.5067]), Grad: tensor([-0.0580,  0.4626])\n",
      "Epoch 83: Loss 0.2669765055179596\n",
      "params: tensor([0.8726, 0.5021]), Grad: tensor([-0.0577,  0.4603])\n",
      "Epoch 84: Loss 0.26482993364334106\n",
      "params: tensor([0.8731, 0.4975]), Grad: tensor([-0.0574,  0.4580])\n",
      "Epoch 85: Loss 0.26270487904548645\n",
      "params: tensor([0.8737, 0.4929]), Grad: tensor([-0.0571,  0.4557])\n",
      "Epoch 86: Loss 0.2606007158756256\n",
      "params: tensor([0.8743, 0.4883]), Grad: tensor([-0.0568,  0.4534])\n",
      "Epoch 87: Loss 0.2585175335407257\n",
      "params: tensor([0.8748, 0.4838]), Grad: tensor([-0.0566,  0.4512])\n",
      "Epoch 88: Loss 0.2564549744129181\n",
      "params: tensor([0.8754, 0.4793]), Grad: tensor([-0.0563,  0.4489])\n",
      "Epoch 89: Loss 0.25441285967826843\n",
      "params: tensor([0.8760, 0.4748]), Grad: tensor([-0.0560,  0.4467])\n",
      "Epoch 90: Loss 0.2523910701274872\n",
      "params: tensor([0.8765, 0.4703]), Grad: tensor([-0.0557,  0.4445])\n",
      "Epoch 91: Loss 0.2503894865512848\n",
      "params: tensor([0.8771, 0.4659]), Grad: tensor([-0.0554,  0.4423])\n",
      "Epoch 92: Loss 0.24840761721134186\n",
      "params: tensor([0.8776, 0.4615]), Grad: tensor([-0.0552,  0.4401])\n",
      "Epoch 93: Loss 0.24644555151462555\n",
      "params: tensor([0.8782, 0.4571]), Grad: tensor([-0.0549,  0.4379])\n",
      "Epoch 94: Loss 0.24450276792049408\n",
      "params: tensor([0.8787, 0.4527]), Grad: tensor([-0.0546,  0.4357])\n",
      "Epoch 95: Loss 0.24257950484752655\n",
      "params: tensor([0.8793, 0.4483]), Grad: tensor([-0.0544,  0.4335])\n",
      "Epoch 96: Loss 0.24067527055740356\n",
      "params: tensor([0.8798, 0.4440]), Grad: tensor([-0.0541,  0.4314])\n",
      "Epoch 97: Loss 0.2387898713350296\n",
      "params: tensor([0.8804, 0.4397]), Grad: tensor([-0.0538,  0.4292])\n",
      "Epoch 98: Loss 0.2369232177734375\n",
      "params: tensor([0.8809, 0.4354]), Grad: tensor([-0.0535,  0.4271])\n",
      "Epoch 99: Loss 0.23507507145404816\n",
      "params: tensor([0.8815, 0.4311]), Grad: tensor([-0.0533,  0.4250])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8820, 0.4269])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0,1.0])\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    w, b  = params\n",
    "    t_p = model(t_u, w, b)\n",
    "    loss = loss_fn(t_p, t_c)\n",
    "    print(f'Epoch {epoch}: Loss {float(loss)}')\n",
    "\n",
    "    grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "\n",
    "    print(f\"params: {params}, Grad: {grad}\")\n",
    "\n",
    "    params = params - learning_rate * grad\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
